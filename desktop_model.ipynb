{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    data = pd.read_csv(\"movie_revenue_data.csv\")\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    years = (data[\"Year\"]).unique()\n",
    "    years.sort()\n",
    "    revenues = []\n",
    "    for year in (data[\"Year\"]).unique():\n",
    "        revenues.append(data[data[\"Year\"] == year][\"WorldwideBox Office\"].mean())\n",
    "\n",
    "    scaled_revenues = np.array(revenues)/1e8\n",
    "    year_revenue_dict = {years[i]: scaled_revenues[i] for i in range(len(years))}\n",
    "    data['Year'] = data['Year'].map(year_revenue_dict)\n",
    "    data[\"Rating\"] = data[\"Rating\"]/10\n",
    "    data[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'WorldwideBox Office']] = np.log2(data[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'WorldwideBox Office']])\n",
    "    data = data[data[\"WorldwideBox Office\"]>13]\n",
    "\n",
    "    import math \n",
    "    SEED = int(math.sqrt(201401004 + 191401009))\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(['Title','WorldwideBox Office'], axis=1), data['WorldwideBox Office'], test_size=0.10, random_state=SEED)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.1111111111111111, random_state=SEED)\n",
    "\n",
    "    from pickle import dump\n",
    "    from sklearn import preprocessing\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]= min_max_scaler.fit_transform(X_train[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']])\n",
    "    X_validation[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]= min_max_scaler.transform(X_validation[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']])\n",
    "    X_test[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']]= min_max_scaler.transform(X_test[['Votes', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']])\n",
    "    dump(min_max_scaler, open('MinMaxScaler.pickle', 'wb'))\n",
    "\n",
    "    return data, X_train, X_validation, X_test, y_train, y_validation, y_test\n",
    "\n",
    "import math \n",
    "SEED = int(math.sqrt(201401004 + 191401009))\n",
    "data, X_train, X_validation, X_test, y_train, y_validation, y_test = set_data()\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    \"kernel\": ['poly', 'rbf'],\n",
    "    \"gamma\": [0.1, 1, 5, 10],\n",
    "    \"degree\": [2, 3, 4],\n",
    "    \"max_iter\": [4000, 5000, 6000, -1]\n",
    "    }\n",
    "SVR = SVR()\n",
    "SVR_grid = GridSearchCV(SVR, grid, refit = True, n_jobs=-1, cv=10)\n",
    "SVR_grid.fit(X_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \", SVR_grid.best_params_)\n",
    "print(\"Support Vector Machine score on train:\", SVR_grid.best_score_)\n",
    "SVR_best = SVR_grid.best_estimator_\n",
    "\n",
    "import pickle\n",
    "pickle.dump(SVR_grid, open('SVR_grid.pickle', 'wb'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "grid = {\n",
    "        \"hidden_layer_sizes\": [(50), (100), (50, 50), (100, 100)],\n",
    "        \"activation\": [\"logistic\", \"tanh\", \"relu\"],\n",
    "        \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "        \"momentum\": [0.6, 0.9],\n",
    "        \"batch_size\": [200, 300, 400],\n",
    "        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "        \"learning_rate_init\": [0.001, 0.005, 0.01],\n",
    "        \"power_t\": [2/3, 1/2, 1/3],\n",
    "        \"max_iter\": [200, 300, 400],\n",
    "        }\n",
    "\n",
    "MLP = MLPClassifier(random_state=SEED, shuffle=True, verbose=1, early_stopping=True)\n",
    "\n",
    "MLP_grid = GridSearchCV(MLP, grid, refit = True, n_jobs=-1, cv=10)\n",
    "MLP_grid.fit(X_train, y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \", MLP_grid.best_params_)\n",
    "print(\"RandomForestRegressor score on train:\", MLP_grid.best_score_)\n",
    "MLP_best = MLP_grid.best_estimator_\n",
    "\n",
    "import pickle\n",
    "pickle.dump(MLP_grid, open('MLP_grid.pickle', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
